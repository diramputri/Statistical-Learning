---
title: "Chapter 6 Problem 8"
author: "Andira Putri"
output:
  pdf_document: default
  html_document: default
---

### In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.

#### a. Use the `rnorm()` function to generate a predictor `X` of length n=100, as well as a noise vector `eps` of length n=100. 

```{r}
set.seed(1)
x=rnorm(100,0,1)
eps=rnorm(100)
```

#### b. Generate a response vector `Y` of length n=100 according to the model:
$Y=\beta_0+\beta_1X+\beta_2X^2+\beta_3X^3+\epsilon$

```{r}
# coefficients are constants of my choice...they are all 1 :-)
Y=1+x+x^2+x^3+eps
plot(x,Y)
```

The model has an apparent cubic form, which makes sense given the equation of Y.

#### c. Use the `regsubsets()` function to perform best subset selection in order to choose the best model containing 10 variables. What is the best model obtained using Cp, BIC, and adjusted R-squared? Show some plots to provide evidence, and report the coefficients of the best model obtained.

For my benefit, I used this as reference on how to use this function: 

http://www2.hawaii.edu/~taylor/z632/Rbestsubsets.pdf

```{r}
df=data.frame(x,Y)
library(leaps)
bss=regsubsets(Y~poly(x,10),data=df,nvmax=10)
sum=summary(bss)

#We use these plots to evaluate Cp, BIC, and adjusted R-sq
plot(bss, scale = "Cp") 
plot(bss, scale = "bic") 
plot(bss, scale = "adjr2")
Cp=which.min(sum$cp) #Choose model w/ lowest Cp
bic=which.min(sum$bic) #Choose model w/ lowest BIC
r2=Cp=which.max(sum$adjr2) #Choose model w/ highest adjR-sq

```


The model of degree 3 is the best according to analysis using BIC, but $C_p$ adjusted $R^2$ suggest that just the model up to degree 5 is the best. 

Combining the results and my intuition, the best model would be the one with degree 3. The coefficients for that model are here:

```{r}
coef(bss,bic)
```


#### d. Repeat (c) using forward and backwards stepwise selection. How does your answer compare to (c)?

```{r}
fwd=regsubsets(Y~poly(x,10),data=df,nvmax=10,method="forward")
sumf=summary(fwd)
which.min(sumf$cp) #Choose model w/ lowest Cp
which.min(sumf$bic) #Choose model w/ lowest BIC
which.max(sumf$adjr2) #Choose model w/ highest adjR-sq

bwd=regsubsets(Y~poly(x,10),data=df,nvmax=10,method="backward")
sumb=summary(bwd)
which.min(sumb$cp) #Choose model w/ lowest Cp
which.min(sumb$bic) #Choose model w/ lowest BIC
which.max(sumb$adjr2) #Choose model w/ highest adjR-sq

```

The results are different from (c) in that both forward and backwards selection suggest the model of degree 4 to be best according to $C_p$. In terms of BIC and adjusted $R^2$ results, the results are the same as (c) for both forward and backwards. BIC suggests model of degree 3 to be best while adj-$R^2$ supports model of degree 5.

#### e. Now fit a lasso model to the simulated data, again using 10 predictors. Use cross-validation to select the optimal value of tuning parameter. Report the resulting coefficient estimates, and discuss the results obtained.

```{r}
set.seed(1)
library(glmnet)
#grid of lambdas from 10^10 to 10^-2
grid=10^seq(5,-2,length=100)
#initialize 10 variables
xtrain=model.matrix(Y~poly(x,10))[,-1]
#for lasso, alpha=1 (0 for ridge reg.)
lasso=glmnet(xtrain,Y,alpha=1,lambda=grid)
plot(lasso)
```

#### f. Now generate a response vector `Y` according to the model below. Perform best subset selection and the lasso. Discuss the results obtained.
$Y=\beta_0+\beta_7X^7+\epsilon$

```{r}
```


